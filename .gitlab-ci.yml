stages:
  - build_and_test

variables:
  GIT_DEPTH: "1"
  GIT_SUBMODULE_STRATEGY: "recursive"
  # uses registry.gitlab.syncad.com/hive/haf/ci-base-image:ubuntu20.04-3
  BUILDER_IMAGE_TAG: "@sha256:a701abca3cdd31bc3a5c7e58a8a2a9ea752e7f30e100aa41bcfe9c56b777883f"
  SETUP_SCRIPTS_PATH: "$CI_PROJECT_DIR/haf/scripts"

hive_fork_manager:
  stage: build_and_test
  image: "$CI_REGISTRY_IMAGE/ci-base-image$BUILDER_IMAGE_TAG"
  script:
    - $SETUP_SCRIPTS_PATH/build.sh --cmake-arg="-DHIVE_LINT=ON" --haf-source-dir="$CI_PROJECT_DIR/haf" --haf-binaries-dir="$CI_PROJECT_DIR/$CI_JOB_NAME/build" extension.hive_fork_manager
    - sudo $SETUP_SCRIPTS_PATH/setup_postgres.sh --haf-admin-account=haf_admin --haf-binaries-dir="$CI_PROJECT_DIR/$CI_JOB_NAME/build"
    - cd "$CI_PROJECT_DIR/$CI_JOB_NAME/build" && ctest --output-on-failure  -R test.functional.hive_fork_manager.*
  artifacts:
    paths:
    - "$CI_JOB_NAME"
    expire_in: 6 hours
  tags:
    - public-runner-docker

# job resonsible for hived build in mainnet config.
hived:
  stage: build_and_test
  image: "$CI_REGISTRY_IMAGE/ci-base-image$BUILDER_IMAGE_TAG"
  script:
    - $SETUP_SCRIPTS_PATH/build.sh --cmake-arg="-DHIVE_LINT=ON" --haf-source-dir="$CI_PROJECT_DIR/haf" --haf-binaries-dir="$CI_PROJECT_DIR/$CI_JOB_NAME/build" hived truncate_block_log
    # check if sql_serializer compiles with hived
    - test -f "$CI_PROJECT_DIR/$CI_JOB_NAME/build/hive/libraries/plugins/sql_serializer/libsql_serializer_plugin.a"
    # check if sql_serializer plugin is included in hived plugins
    - cd "$CI_PROJECT_DIR/$CI_JOB_NAME/build/hive/programs/hived"
    - ./hived --help | grep psql-url
  artifacts:
    paths:
    - "$CI_JOB_NAME"
    expire_in: 6 hours
  tags:
    - public-runner-docker

patterns_tests:
  stage: build_and_test
  needs:
    - job: hive_fork_manager
      artifacts: true
    - job: hived
      artifacts: true
  image: "$CI_REGISTRY_IMAGE/ci-base-image$BUILDER_IMAGE_TAG"
  variables:
    HIVE_BUILD_ROOT_PATH: "$CI_PROJECT_DIR/hived/build/hive"
    BLOCK_LOG_PATH: "/blockchain/block_log"
  script:
    - pip3 install -r requirements.txt

    # install hive_fork_manager extension built in previous stage
    # replay and prepare database
    - if [ ! "$POSTGRESQL_URI" ] && [ ! "$AH_ENDPOINT" ]; then
        sudo $SETUP_SCRIPTS_PATH/setup_postgres.sh --haf-admin-account=haf_admin --haf-binaries-dir="$CI_PROJECT_DIR/hive_fork_manager/build" ;
        $SETUP_SCRIPTS_PATH/setup_db.sh --haf-db-name="haf_block_log" --haf-db-owner="hive" ;
        mkdir $CI_PROJECT_DIR/tests/prepare_database/blockchain ;
        $HIVE_BUILD_ROOT_PATH/programs/util/truncate_block_log $BLOCK_LOG_PATH $CI_PROJECT_DIR/tests/prepare_database/blockchain/block_log 5000000 ;
        ($HIVE_BUILD_ROOT_PATH/programs/hived/hived --data-dir $CI_PROJECT_DIR/tests/prepare_database --config config.ini --force-replay --stop-replay-at 5000000 2>&1 | tee -i $CI_PROJECT_DIR/from_node.log &) | grep -q "Entering application main loop";
        export POSTGRESQL_URI='postgresql:///haf_block_log' ;
      fi

    # run HAfAH
    - if [ ! "$AH_ENDPOINT" ]; then
        cd $CI_PROJECT_DIR ;
        ./main.py --psql-db-path $POSTGRESQL_URI --port 6543 >> ah.log &
        export AH_ENDPOINT=localhost:6543 ;
      fi

    # run pattern tests
    - cd $CI_PROJECT_DIR/haf/hive/tests/api_tests
    - ./run_tests.sh $AH_ENDPOINT `git rev-parse --show-toplevel`

    # run comparsion tests
    - export AHRB_PORT=$(cat $CI_PROJECT_DIR/tests/prepare_database/config.ini | grep webserver-http-endpoint | cut -d ':' -f 2)
    - export TJOBS=10
    - cd $CI_PROJECT_DIR/haf/hive/tests/tests_api/hived
    - ./test_ah_enum_virtual_ops.py --ref http://localhost:$AHRB_PORT --test http://$AH_ENDPOINT --start 4900000 --stop 4925000 -d workdir_enum -j $TJOBS
    - ./test_ah_get_ops_in_block.py --ref http://localhost:$AHRB_PORT --test http://$AH_ENDPOINT --start 4900000 --stop 4925000 -d workdir_ops -j $TJOBS
    - ./test_ah_get_account_history.py --ref http://localhost:$AHRB_PORT --test http://$AH_ENDPOINT -f ./input_data/accounts.csv -d workdir_acc -j $TJOBS
    - ./test_ah_get_transaction.py --ref http://localhost:$AHRB_PORT --test http://$AH_ENDPOINT -f ./input_data/hashes.csv -d workdir_trx -j $TJOBS

    # kill hived
    - pkill hived

  artifacts:
    when: always
    reports:
      junit: $CI_PROJECT_DIR/haf/hive/tests/api_tests/results.xml
    paths:
    - "$CI_JOB_NAME"
    - "**/from_node.log"
    - "**/ah.log"
    - "**/*.out.json"
    - "$CI_PROJECT_DIR/haf/hive/tests/tests_api/hived/workdir_*"
    when: always
    expire_in: 6 hours
  tags:
    - public-runner-docker
    - hived-for-tests
