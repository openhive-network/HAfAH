stages:
  - build
  - test
  - cleanup

variables:
  GIT_DEPTH: 20
  GIT_SUBMODULE_STRATEGY: "recursive"
  CI_DEBUG_SERVICES: "true"
  # uses registry.gitlab.syncad.com/hive/haf/ci-base-image:ubuntu20.04-5
  BUILDER_IMAGE_TAG: "@sha256:6b557af6a98188c118d442b68437bf1b20d67e3aae5765269c4fe88465c62d60"
  SETUP_SCRIPTS_PATH: "$CI_PROJECT_DIR/haf/scripts"
  REGISTRY: registry.gitlab.syncad.com/hive/hafah
  APP_PORT: 6543
  HAF_POSTGRES_URL: postgresql://haf_app_admin@haf-instance:5432/haf_block_log
  BENCHMARK_SOURCE_DIR: "$CI_PROJECT_DIR/haf/hive/tests/tests_api/benchmarks"

  # Variables required by Common CI jobs
  CI_COMMON_JOB_VERSION: "a32b4a361b94287eadeea1104542e4d1b0e4c362"
  IMAGE_REMOVER_TAG: "$CI_COMMON_JOB_VERSION"
  TOX_IMAGE_TAG: "$CI_COMMON_JOB_VERSION"
  
  # Variables specific to runner (there is runner cache and there is 5m block_log available)
  DATA_CACHE_HIVE: /cache/replay_data_hafah_hived_${CI_PIPELINE_ID}
  DATA_CACHE_HAF: /cache/replay_data_hafah_haf_${CI_PIPELINE_ID}
  BLOCK_LOG_SOURCE_DIR_5M: /blockchain/block_log_5m

include:
  - template: Workflows/Branch-Pipelines.gitlab-ci.yml
  - project: 'hive/haf'
    ref: ms_setup_ci_using_cache
    file: '/scripts/ci-helpers/prepare_data_image_job.yml'
  - project: 'hive/hive'
    ref: ms_setup_ci_using_cache
    file: '/scripts/ci-helpers/prepare_data_image_job.yml'
  - project: 'hive/common-ci-configuration'
    ref: a32b4a361b94287eadeea1104542e4d1b0e4c362 # It seems a variable cannot be used here
    file: '/templates/test_jobs.gitlab-ci.yml'

prepare_hived_image:
  extends: .prepare_hived_image
  stage: build
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/haf/hive"
    REGISTRY_USER: "$HIVED_CI_IMGBUILDER_USER"
    REGISTRY_PASS: $HIVED_CI_IMGBUILDER_PASSWORD

  tags:
    - public-runner-docker
    - hived-for-tests

prepare_hived_data:
  extends: .prepare_hived_data_5m
  needs:
    - job: prepare_hived_image
      artifacts: true
  stage: build
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/haf/hive"
    DATA_CACHE: "$DATA_CACHE_HIVE"
    BLOCK_LOG_SOURCE_DIR: $BLOCK_LOG_SOURCE_DIR_5M
    CONFIG_INI_SOURCE: "$CI_PROJECT_DIR/haf/hive/docker/config_5M.ini"
  tags:
    - hive-builder-4

prepare_haf_image:
  extends: .prepare_haf_image
  stage: build
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/haf"
    REGISTRY_USER: "$CI_IMG_BUILDER_USER"
    REGISTRY_PASS: $CI_IMG_BUILDER_PASSWORD

  tags:
    - public-runner-docker
    - hived-for-tests

prepare_haf_data:
  extends: .prepare_haf_data_5m
  needs:
    - job: prepare_haf_image
      artifacts: true
  stage: build
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/haf"
    DATA_CACHE: $DATA_CACHE_HAF
    BLOCK_LOG_SOURCE_DIR: $BLOCK_LOG_SOURCE_DIR_5M
    CONFIG_INI_SOURCE: "$CI_PROJECT_DIR/haf/docker/config_5M.ini"
  tags:
    - hive-builder-4

.prepare_hafah_image:
  extends: .docker_image_builder_job
  stage: build
  variables:
    HAFAH_IMAGE_NAME: ""
    USE_POSTGREST: 0

  script:
    - docker build --build-arg USE_POSTGREST=$USE_POSTGREST --build-arg HTTP_PORT=$APP_PORT --build-arg POSTGRES_URL="$HAF_POSTGRES_URL" --target=instance -t $HAFAH_IMAGE_NAME -f Dockerfile .
    - echo $HAFAH_CI_IMG_BUILDER_PASSWORD | docker login -u $HAFAH_CI_IMG_BUILDER_USER $REGISTRY --password-stdin
    - docker push $HAFAH_IMAGE_NAME
    - echo "HAFAH_IMAGE_NAME=$HAFAH_IMAGE_NAME" > docker_image_name.env

  artifacts:
    reports:
      dotenv: docker_image_name.env
    expire_in: 6 hours

  tags:
    - public-runner-docker
    - hived-for-tests

prepare_python_hafah_image:
  extends: .prepare_hafah_image
  variables:
    HAFAH_IMAGE_NAME: $REGISTRY/python-instance:$CI_COMMIT_SHORT_SHA
    USE_POSTGREST: 0

prepare_postgrest_hafah_image:
  extends: .prepare_hafah_image
  variables:
    HAFAH_IMAGE_NAME: $REGISTRY/postgrest-instance:$CI_COMMIT_SHORT_SHA
    USE_POSTGREST: 1

.pattern_tests:
  extends: .pattern_tests_template
  stage: test
  variables:
    FF_NETWORK_PER_BUILD: 1
    PYTHONPATH: "$CI_PROJECT_DIR/tests/test-tools/package"
    APP_IMAGE: ""
    TEST_SUITE: "condenser_api_patterns/get_transaction and not get_transaction_hex or account_history_api or condenser_api_patterns/get_account_history or condenser_api_patterns/get_ops_in_block"
    DIRECT_HAFAH_CALLS: 0
    ENDPOINT: app:$APP_PORT
  services:
    - name: $HAF_IMAGE_NAME
      alias: haf-instance
      variables:
        # Allow access from any network to eliminate CI IP addressing problems
        PG_ACCESS: "host    haf_block_log    haf_app_admin    0.0.0.0/0    trust"
        DATADIR: $DATA_CACHE_HAF/datadir
        SHM_DIR: $DATA_CACHE_HAF/shm_dir
        LOG_FILE: $CI_JOB_NAME.log
      command: ["--replay-blockchain", "--stop-replay-at-block=5000000", "--data-dir=${DATA_CACHE_HAF}/datadir", "--shared-file-dir=${DATA_CACHE_HAF}/shm_dir"]
    - name: $APP_IMAGE
      alias: app

  needs:
    - job: prepare_haf_image
      artifacts: true
    - job: prepare_haf_data

  before_script:
    - echo "HAfAH image name $APP_IMAGE"
    - echo "HAF image name $HAF_IMAGE_NAME"

  script:
    - pip3 install -r $CI_PROJECT_DIR/haf/hive/tests/api_tests/comparsion_tests/requirements.txt
    # run pattern tests
    - cd $CI_PROJECT_DIR/haf/hive/tests/api_tests/pattern_tests
    - ./run_tests.sh $ENDPOINT `git rev-parse --show-toplevel` "${TEST_SUITE}" ${DIRECT_HAFAH_CALLS}

  artifacts:
    paths:
    - "$CI_JOB_NAME"
    - "**/from_node.log"
    - "**/ah.log"
    - "**/*.out.json"
    - "tests/tests_api/hived/workdir_*"
    - "tests/api_tests/pattern_tests/results.xml"
    reports:
      junit: haf/hive/tests/api_tests/pattern_tests/results.xml
  resource_group: ${CI_COMMIT_SHA}
  tags:
    - hive-builder-4

python_pattern_tests:
  extends: .pattern_tests

  needs:
    - !reference [.pattern_tests, needs]
    - job: prepare_python_hafah_image
      artifacts: true

  variables:
    APP_IMAGE: $HAFAH_IMAGE_NAME

postgrest_pattern_tests:
  extends: .pattern_tests

  needs:
    - !reference [.pattern_tests, needs]
    - job: prepare_postgrest_hafah_image
      artifacts: true

  variables:
    APP_IMAGE: $HAFAH_IMAGE_NAME

new_style_postgrest_pattern_tests:
  extends: .pattern_tests

  needs:
    - !reference [.pattern_tests, needs]
    - job: prepare_postgrest_hafah_image
      artifacts: true

  variables:
    APP_IMAGE: $HAFAH_IMAGE_NAME
    # Direct call version does not support condenser_api
    TEST_SUITE: "account_history_api"
    DIRECT_HAFAH_CALLS: 1

.comparison_tests:
  extends: .comparison_tests_template
  stage: test
  variables:
    FF_NETWORK_PER_BUILD: 1
    PYTHONPATH: "$CI_PROJECT_DIR/tests/test-tools/package"
    APP_IMAGE: ""

  needs:
    - job: prepare_haf_image
      artifacts: true
    - job: prepare_haf_data
    - job: prepare_hived_image
      artifacts: true
    - job: prepare_hived_data
  services:
    - name: $HAF_IMAGE_NAME
      alias: haf-instance
      variables:
        # Allow access from any network to eliminate CI IP addressing problems
        PG_ACCESS: "host    haf_block_log    haf_app_admin    0.0.0.0/0    trust"
        DATADIR: $DATA_CACHE_HAF/datadir
        SHM_DIR: $DATA_CACHE_HAF/shm_dir
        LOG_FILE: $CI_JOB_NAME.log
      command: ["--replay-blockchain", "--stop-replay-at-block=5000000", "--data-dir=${DATA_CACHE_HAF}/datadir", "--shared-file-dir=${DATA_CACHE_HAF}/shm_dir"]
    - name: $APP_IMAGE
      alias: app
    - name: $HIVED_IMAGE_NAME
      alias: hived-instance
      variables:
        DATADIR: $DATA_CACHE_HIVE/datadir
        SHM_DIR: $DATA_CACHE_HIVE/shm_dir
        LOG_FILE: $CI_JOB_NAME.log
      command: ["--replay-blockchain", "--stop-replay-at-block=5000000", "--data-dir=${DATA_CACHE_HIVE}/datadir", "--shared-file-dir=${DATA_CACHE_HIVE}/shm_dir"]

  script:
    - (nc -z haf-instance 5432 ) || echo "haf-instance postgres port not open"
    - pip3 install -r $CI_PROJECT_DIR/haf/hive/tests/api_tests/comparsion_tests/requirements.txt "$CI_PROJECT_DIR/tests/test-tools"
      # run comparsion tests
    - cd $CI_PROJECT_DIR/haf/hive/tests/api_tests/comparsion_tests
    - python3 -m pytest -n 8 --ref http://$HIVED_ENDPOINT --test http://$ENDPOINT --start 4900000 --stop 4915000 --junitxml=$CI_PROJECT_DIR/comparsion_tests.xml

  artifacts:
    paths:
    - "$CI_JOB_NAME"
    - "**/from_node.log"
    - "**/ah.log"
    - "**/*.out.json"
    - "haf/hive/tests/tests_api/hived/workdir_*"
    - "comparsion_tests.xml"
    reports:
      junit:
        - comparsion_tests.xml
  resource_group: ${CI_COMMIT_SHA}
  tags:
    - hive-builder-4

python_comparison_tests:
  extends: .comparison_tests

  needs:
    - !reference [.comparison_tests, needs]
    - job: prepare_python_hafah_image
      artifacts: true

  variables:
    APP_IMAGE: $HAFAH_IMAGE_NAME

postgrest_comparison_tests:
  extends: .comparison_tests

  needs:
    - !reference [.comparison_tests, needs]
    - job: prepare_postgrest_hafah_image
      artifacts: true

  variables:
    APP_IMAGE: $HAFAH_IMAGE_NAME

.benchmark_tests:
  extends: .jmeter_benchmark_with_haf_job
  stage: test
  variables:
    FF_NETWORK_PER_BUILD: 1
    API_FOR_TESTING: "account_history_api" # alternatively: blocks_api
    PYTHONPATH: "$CI_PROJECT_DIR/tests/test-tools/package"
    APP_IMAGE: ""
  needs:
    - job: prepare_haf_image
      artifacts: true
    - job: prepare_haf_data
  services:
    - name: $HAF_IMAGE_NAME
      alias: haf-instance
      variables:
        # Allow access from any network to eliminate CI IP addressing problems
        PG_ACCESS: "host    haf_block_log    haf_app_admin    0.0.0.0/0    trust"
        DATADIR: $DATA_CACHE_HAF/datadir
        SHM_DIR: $DATA_CACHE_HAF/shm_dir
        LOG_FILE: $CI_JOB_NAME.log
      command: ["--replay-blockchain", "--stop-replay-at-block=5000000", "--data-dir=${DATA_CACHE_HAF}/datadir", "--shared-file-dir=${DATA_CACHE_HAF}/shm_dir"]
    - name: $APP_IMAGE
      alias: app

  script:
      - /usr/bin/python3 $CI_PROJECT_DIR/tests/tests_api/benchmarks/benchmark.py -a app -p $APP_PORT -c perf_5M_heavy.csv -d $CI_PROJECT_DIR/wdir -n $API_FOR_TESTING
      - m2u --input wdir/raw_jmeter_report.xml --output wdir/jmeter_junit_report.xml
      - jmeter -g wdir/jmeter_${APP_PORT}_output.csv -o wdir/dashboard/
  artifacts:
    paths:
      - wdir/
    reports:
      junit: wdir/jmeter_junit_report.xml
  resource_group: ${CI_COMMIT_SHA}
  tags:
    - hive-builder-4

python_account_history_benchmark_tests:
  extends: .benchmark_tests

  needs:
    - !reference [.benchmark_tests, needs]
    - job: prepare_python_hafah_image
      artifacts: true

  variables:
    APP_IMAGE: $HAFAH_IMAGE_NAME

postgrest_block_api_benchmark_tests:
  extends: .benchmark_tests

  needs:
    - !reference [.benchmark_tests, needs]
    - job: prepare_postgrest_hafah_image
      artifacts: true

  variables:
    APP_IMAGE: $HAFAH_IMAGE_NAME
    API_FOR_TESTING: blocks_api

postgrest_account_history_benchmark_tests:
  extends: .benchmark_tests

  needs:
    - !reference [.benchmark_tests, needs]
    - job: prepare_postgrest_hafah_image
      artifacts: true

  variables:
    APP_IMAGE: $HAFAH_IMAGE_NAME


cleanup_cache_manual:
  extends: .docker_image_cleanup_job
  needs: []
  stage: cleanup
  script:
    - echo "HOSTNAME is ${HOSTNAME} CI_CONCURRENT_ID ${CI_CONCURRENT_ID}"
    - du -h -d 1 /cache
    - rm /cache/replay_data_hafah_* -rf
    - rm /cache/replay_data_hivemind_* -rf
  resource_group: ${CI_COMMIT_SHA}
  tags:
    - hive-builder-4
  when: manual
