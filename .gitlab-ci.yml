stages:
  - build
  - test
  - publish
  - cleanup

variables:
  GIT_DEPTH: 20
  GIT_STRATEGY: clone
  GIT_SUBMODULE_STRATEGY: "recursive"
  CI_DEBUG_SERVICES: "true"
  # uses registry.gitlab.syncad.com/hive/haf/ci-base-image:ubuntu20.04-5
  BUILDER_IMAGE_TAG: "@sha256:6b557af6a98188c118d442b68437bf1b20d67e3aae5765269c4fe88465c62d60"
  SETUP_SCRIPTS_PATH: "$CI_PROJECT_DIR/haf/scripts"
  REGISTRY: registry.gitlab.syncad.com/hive/hafah
  APP_PORT: 6543
  HAF_POSTGRES_URL: postgresql://haf_app_admin@haf-instance:5432/haf_block_log
  BENCHMARK_SOURCE_DIR: "$CI_PROJECT_DIR/haf/hive/tests/tests_api/benchmarks"

  # Variables required by Common CI jobs
  TOX_IMAGE_TAG: "59834935f116dce106fdf9fb98e44a6f646f5eba"
  PYTEST_NUMBER_OF_PROCESSES: 8
  HAFAH_BUILD_DIR: "$CI_PROJECT_DIR/build"
  BUILDER_IMAGE_PATH: "registry.gitlab.syncad.com/hive/haf/ci-base-image@sha256:b639fee3a6f7761a8e38777045e65bc012fbd2ad01bf366e5e9211ac765f9629"

  # Variables specific to runner (there is single runner cache and there is 5m block_log available)
  DATA_CACHE_HIVE_TEMPLATE: /cache/replay_data_hafah_hived
  DATA_CACHE_HAF_TEMPLATE: /cache/replay_data_hafah_haf
  DATA_CACHE_HIVE: "/cache/replay_data_hafah_hive_${CI_PIPELINE_ID}"
  DATA_CACHE_HAF: "/cache/replay_data_hafah_haf_${CI_PIPELINE_ID}"
  BLOCK_LOG_SOURCE_DIR_5M: /blockchain/block_log_5m

include:
  - template: Workflows/Branch-Pipelines.gitlab-ci.yml
  - project: 'hive/haf'
    ref: 94183788111c2f6f650859ad11bebbf867aa5010 # develop
    file: '/scripts/ci-helpers/prepare_data_image_job.yml'
  - project: 'hive/common-ci-configuration'
    ref: 59834935f116dce106fdf9fb98e44a6f646f5eba # It seems a variable cannot be used here
    file: '/templates/test_jobs.gitlab-ci.yml'

prepare_hived_image:
  extends: .prepare_hived_image
  stage: build
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/haf/hive"
    REGISTRY_USER: "$HIVED_CI_IMGBUILDER_USER"
    REGISTRY_PASS: $HIVED_CI_IMGBUILDER_PASSWORD
  tags:
    - public-runner-docker
    - hived-for-tests

prepare_hived_data:
  extends: .prepare_hived_data_5m
  needs:
    - job: prepare_hived_image
      artifacts: true
  stage: build
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/haf/hive"
    BLOCK_LOG_SOURCE_DIR: $BLOCK_LOG_SOURCE_DIR_5M
    CONFIG_INI_SOURCE: "$CI_PROJECT_DIR/haf/hive/docker/config_5M.ini"
  resource_group: ${CI_COMMIT_SHA}
  tags:
    - data-cache-storage

prepare_haf_image:
  extends: .prepare_haf_image
  stage: build
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/haf"
    REGISTRY_USER: "$CI_IMG_BUILDER_USER"
    REGISTRY_PASS: $CI_IMG_BUILDER_PASSWORD
  tags:
    - public-runner-docker
    - hived-for-tests

prepare_haf_data:
  extends: .prepare_haf_data_5m
  needs:
    - job: prepare_haf_image
      artifacts: true
  stage: build
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/haf"
    BLOCK_LOG_SOURCE_DIR: $BLOCK_LOG_SOURCE_DIR_5M
    CONFIG_INI_SOURCE: "$CI_PROJECT_DIR/haf/docker/config_5M.ini"
  resource_group: ${CI_COMMIT_SHA}
  tags:
    - data-cache-storage

.prepare_hafah_image:
  extends: .docker_image_builder_job
  stage: build
  variables:
    HAFAH_IMAGE_TAG: ""
    HAFAH_IMAGE_NAME: $CI_REGISTRY_IMAGE/instance:instance-$HAFAH_IMAGE_TAG
    USE_POSTGREST: 0
    SOURCE_DIR: $CI_PROJECT_DIR

  script:
    - echo $HAFAH_IMAGE_NAME
    - echo $SOURCE_DIR
    - ls -la
    - pwd
    - |
      scripts/ci-helpers/build_instance.sh
      docker login -u "$HAFAH_CI_IMG_BUILDER_USER" -p "$HAFAH_CI_IMG_BUILDER_PASSWORD" "$REGISTRY"
      docker push "$HAFAH_IMAGE_NAME"
      echo "HAFAH_IMAGE_NAME=$HAFAH_IMAGE_NAME" > docker_image_name.env

  artifacts:
    reports:
      dotenv: docker_image_name.env
    expire_in: 6 hours

  tags:
    - public-runner-docker
    - hived-for-tests

# prepare_python_hafah_image:
#   extends: .prepare_hafah_image
#   variables:
#     HAFAH_IMAGE_TAG: python-$CI_COMMIT_SHORT_SHA
#     USE_POSTGREST: 0

prepare_postgrest_hafah_image:
  extends: .prepare_hafah_image
  variables:
    HAFAH_IMAGE_TAG: postgrest-$CI_COMMIT_SHORT_SHA
    USE_POSTGREST: 1

.pattern_tests:
  extends: .pattern_tests_template
  stage: test
  variables:
    FF_NETWORK_PER_BUILD: 1
    PYTHONPATH: "$CI_PROJECT_DIR/haf/hive/tests/hive-local-tools/test-tools/package"
    APP_IMAGE: ""
    TEST_SUITE: "condenser_api_patterns/get_transaction and not get_transaction_hex or account_history_api or condenser_api_patterns/get_account_history or condenser_api_patterns/get_ops_in_block"
    DIRECT_HAFAH_CALLS: 0
    HIVED_UID: $HIVED_UID

  needs:
    - job: prepare_haf_data
      artifacts: true

  before_script:
    - echo "HAfAH image name $APP_IMAGE"
    - echo "HAF image name $HAF_IMAGE_NAME"

  script:
    - pip3 install -r $CI_PROJECT_DIR/haf/hive/tests/api_tests/comparsion_tests/requirements.txt
    # run pattern tests
    - cd $CI_PROJECT_DIR/haf/hive/tests/api_tests/pattern_tests
    - ./run_tests.sh $ENDPOINT `git rev-parse --show-toplevel` "${TEST_SUITE}" ${DIRECT_HAFAH_CALLS}

  artifacts:
    paths:
    - "$CI_JOB_NAME"
    - "**/from_node.log"
    - "**/ah.log"
    - "**/*.out.json"
    - "tests/tests_api/hived/workdir_*"
    - "tests/api_tests/pattern_tests/results.xml"
    reports:
      junit: haf/hive/tests/api_tests/pattern_tests/results.xml
  resource_group: ${CI_COMMIT_SHA}
  tags:
    - data-cache-storage

# python_pattern_tests:
#   extends: .pattern_tests

#   needs:
#     - !reference [.pattern_tests, needs]
#     - job: prepare_python_hafah_image
#       artifacts: true

#   variables:
#     APP_IMAGE: $HAFAH_IMAGE_NAME

# postgrest_pattern_tests:
#   extends: .pattern_tests

#   needs:
#     - !reference [.pattern_tests, needs]
#     - job: prepare_postgrest_hafah_image
#       artifacts: true

#   variables:
#     APP_IMAGE: $HAFAH_IMAGE_NAME

# new_style_postgrest_pattern_tests:
#   extends: .pattern_tests

#   needs:
#     - !reference [.pattern_tests, needs]
#     - job: prepare_postgrest_hafah_image
#       artifacts: true

#   variables:
#     APP_IMAGE: $HAFAH_IMAGE_NAME
#     # Direct call version does not support condenser_api
#     TEST_SUITE: "account_history_api"
#     DIRECT_HAFAH_CALLS: 1

# .comparison_tests:
#   extends: .comparison_tests_template
#   stage: test
#   variables:
#     FF_NETWORK_PER_BUILD: 1
#     PYTHONPATH: "$CI_PROJECT_DIR/tests/test-tools/package"
#     HIVED_UID: $HIVED_UID

#   needs:
#     - job: prepare_haf_data
#       artifacts: true
#     - job: prepare_hived_data
#       artifacts: true

#   script:
#     - pip3 install -r $CI_PROJECT_DIR/haf/hive/tests/api_tests/comparsion_tests/requirements.txt "$CI_PROJECT_DIR/haf/hive/tests/hive-local-tools/test-tools"
#       # run comparsion tests
#     - cd $CI_PROJECT_DIR/haf/hive/tests/api_tests/comparsion_tests
#     - python3 -m pytest -n 8 --ref http://$HIVED_ENDPOINT --test http://$ENDPOINT --start 4900000 --stop 4915000 --junitxml=$CI_PROJECT_DIR/comparsion_tests.xml

#   artifacts:
#     paths:
#     - "$CI_JOB_NAME"
#     - "**/from_node.log"
#     - "**/ah.log"
#     - "**/*.out.json"
#     - "haf/hive/tests/tests_api/hived/workdir_*"
#     - "comparsion_tests.xml"
#     reports:
#       junit:
#         - comparsion_tests.xml
#   resource_group: ${CI_COMMIT_SHA}
#   tags:
#     - data-cache-storage

# python_comparison_tests:
#   extends: .comparison_tests

#   needs:
#     - !reference [.comparison_tests, needs]
#     - job: prepare_python_hafah_image
#       artifacts: true

#   variables:
#     APP_IMAGE: $HAFAH_IMAGE_NAME

# postgrest_comparison_tests:
#   extends: .comparison_tests

#   needs:
#     - !reference [.comparison_tests, needs]
#     - job: prepare_postgrest_hafah_image
#       artifacts: true

#   variables:
#     APP_IMAGE: $HAFAH_IMAGE_NAME

# .benchmark_tests:
#   extends: .jmeter_benchmark_with_haf_job
#   stage: test
#   variables:
#     FF_NETWORK_PER_BUILD: 1
#     API_FOR_TESTING: "account_history_api" # alternatively: blocks_api
#     PYTHONPATH: "$CI_PROJECT_DIR/haf/hive/tests/hive-local-tools/test-tools/package"
#     APP_IMAGE: ""
#     HIVED_UID: $HIVED_UID
#   needs:
#     - job: prepare_haf_data
#       artifacts: true

#   script:
#       - /usr/bin/python3 $CI_PROJECT_DIR/tests/integration/hafah-local-tools/tests_api/benchmarks/benchmark.py -a app -p $APP_PORT -c perf_5M_heavy.csv -d $CI_PROJECT_DIR/wdir -n $API_FOR_TESTING
#       - m2u --input wdir/raw_jmeter_report.xml --output wdir/jmeter_junit_report.xml
#       - jmeter -g wdir/jmeter_${APP_PORT}_output.csv -o wdir/dashboard/
#   artifacts:
#     paths:
#       - wdir/
#     reports:
#       junit: wdir/jmeter_junit_report.xml
#   resource_group: ${CI_COMMIT_SHA}
#   tags:
#     - data-cache-storage

# python_account_history_benchmark_tests:
#   extends: .benchmark_tests

#   needs:
#     - !reference [.benchmark_tests, needs]
#     - job: prepare_python_hafah_image
#       artifacts: true

#   variables:
#     APP_IMAGE: $HAFAH_IMAGE_NAME

# postgrest_block_api_benchmark_tests:
#   extends: .benchmark_tests

#   needs:
#     - !reference [.benchmark_tests, needs]
#     - job: prepare_postgrest_hafah_image
#       artifacts: true

#   variables:
#     APP_IMAGE: $HAFAH_IMAGE_NAME
#     API_FOR_TESTING: blocks_api

# postgrest_account_history_benchmark_tests:
#   extends: .benchmark_tests

#   needs:
#     - !reference [.benchmark_tests, needs]
#     - job: prepare_postgrest_hafah_image
#       artifacts: true

#   variables:
#     APP_IMAGE: $HAFAH_IMAGE_NAME

cleanup_hived_cache_manual:
  extends: .cleanup_cache_manual
  stage: cleanup
  variables:
    CLEANUP_PATH_PATTERN: "/cache/replay_data_hafah_*"
  resource_group: ${CI_COMMIT_SHA}
  tags:
    - data-cache-storage

build_and_publish_image:
  stage: publish
  extends: .publish_docker_image_template
  script:
    - scripts/ci-helpers/build_and_publish_instance.sh
  tags:
    - public-runner-docker

prepare_haf_image_testnet:
  extends: .prepare_haf_image
  stage: build
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/haf"
    REGISTRY_USER: "$CI_IMG_BUILDER_USER"
    REGISTRY_PASS: $CI_IMG_BUILDER_PASSWORD
    HIVE_NETWORK_TYPE: testnet
    BINARY_CACHE_PATH: "haf-testnet-binaries"
  tags:
    - public-runner-docker
    - hived-for-tests

.hfm-only-service: &hfm-only-service
  name: $HAF_IMAGE_NAME
  alias: hfm-only-instance
  variables:
    # Allow access from any network to eliminate CI IP addressing problems when hfm runs as service
    PG_ACCESS: |
                "host    all              haf_admin        0.0.0.0/0    trust"
                "host    all              haf_app_admin    0.0.0.0/0    trust"
                "host    all              all              0.0.0.0/0    scram-sha-256"
  command: ["--postgres"]

.pytest_based:
  before_script:
    # - export PATH="/home/haf_admin/.local/bin:$PATH"
    - curl -sSL https://install.python-poetry.org | python3 -  # install poetry in isolated environment
    - python3 -m venv venv/
    - . venv/bin/activate
    - (cd $CI_PROJECT_DIR/haf/hive/tests/hive-local-tools && poetry install)
    - (cd $CI_PROJECT_DIR/tests/integration/hafah-local-tools && poetry install)

.hafah_pytest_fuctional_tests_base:
  stage: test
  extends: .pattern_tests
  needs:
    - !reference [.pattern_tests, needs]
    - job: prepare_haf_image_testnet
      artifacts: true
    - job: prepare_postgrest_hafah_image
      artifacts: true
  image: $BUILDER_IMAGE_PATH
  services:
    - *hfm-only-service
    - name: $APP_IMAGE
      alias: app
  variables:
    BINARY_CACHE_PATH: "haf-testnet-binaries"
    HIVED_PATH: "$CI_PROJECT_DIR/$BINARY_CACHE_PATH/hived"
    COMPRESS_BLOCK_LOG_PATH: "$CI_PROJECT_DIR/$BINARY_CACHE_PATH/compress_block_log"
    GET_DEV_KEY_PATH: "$CI_PROJECT_DIR/$BINARY_CACHE_PATH/get_dev_key"
    CLI_WALLET_PATH: "$CI_PROJECT_DIR/$BINARY_CACHE_PATH/cli_wallet"
    DB_NAME: haf_block_log
    DB_URL: "postgresql://haf_admin@hfm-only-instance:5432/$DB_NAME"
    APP_IMAGE: $HAFAH_IMAGE_NAME
    POSTGRES_URL: $DB_URL
    PYTEST_MARK:
  script:
      # - export PATH="/home/haf_admin/.local/bin:$PATH"
    - curl -sSL https://install.python-poetry.org | python3 -  # install poetry in isolated environment
    - python3 -m venv venv/
    - . venv/bin/activate
    - (cd $CI_PROJECT_DIR/haf/hive/tests/hive-local-tools && poetry install)
    - (cd $CI_PROJECT_DIR/tests/integration/hafah-local-tools && poetry install)


    # setup postgres and DB for work with hafah
    - $CI_PROJECT_DIR/scripts/setup_postgres.sh --postgres-url="$DB_URL"
    - bash ${CI_PROJECT_DIR}/scripts/generate_version_sql.bash ${CI_PROJECT_DIR}
    - $CI_PROJECT_DIR/scripts/setup_db.sh --postgres-url="$DB_URL"

    # run tests
    - cd $CI_PROJECT_DIR/tests/integration/functional
    - pytest --junitxml report.xml --postgrest-hafah-adress=app:$APP_PORT --postgres-db-url=$DB_URL -m $PYTEST_MARK
  artifacts:
    paths:
    - "**/generated_during_*"
    - "**/generated_by_package_fixtures"
    reports:
      junit: tests/integration/system/haf/report.xml
    when: always
    expire_in: 1 week
  interruptible: true
  tags:
    - hived-for-tests

hafah_pytest_fuctional_tests_part1:
  extends: .hafah_pytest_fuctional_tests_base
  variables:
    PYTEST_MARK: "enum_virtual_ops_and_get_ops_in_block"

hafah_pytest_fuctional_tests_part2:
  extends: .hafah_pytest_fuctional_tests_base
  variables:
    PYTEST_MARK: "get_account_history_and_get_transaction"
