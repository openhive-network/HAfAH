stages:
  - build_and_test_phase_1
  - build_and_test_phase_2
  - docker_build
  - deploy
  - cleanup
  - publish

variables:
  PYTEST_NUMBER_OF_PROCESSES: 8
  CTEST_NUMBER_OF_JOBS: 4
  GIT_DEPTH: 20
  GIT_SUBMODULE_STRATEGY: recursive
  FF_ENABLE_JOB_CLEANUP: 1
  GIT_STRATEGY: clone
  # uses registry.gitlab.syncad.com/hive/haf/ci-base-image:ubuntu22.04-3
  BUILDER_IMAGE_TAG: "@sha256:b639fee3a6f7761a8e38777045e65bc012fbd2ad01bf366e5e9211ac765f9629"
  CI_DEBUG_SERVICES: "true"
  SETUP_SCRIPTS_PATH: "$CI_PROJECT_DIR/scripts"
  TEST_TOOLS_NODE_DEFAULT_WAIT_FOR_LIVE_TIMEOUT: 60
  DATA_CACHE_HAF_TEMPLATE: /cache/replay_data_haf
  DATA_CACHE_HAF: "/cache/replay_data_haf_${CI_PIPELINE_ID}"
  BLOCK_LOG_SOURCE_DIR_5M: /blockchain/block_log_5m
  SNAPSHOTS_PATH: /cache/snapshots_pipeline_${CI_PIPELINE_ID}
  BLOCK_LOG_SOURCE_DIR_MIRRORNET_5M: /cache/block_log_5m_mirrornet

include:
  - template: Workflows/Branch-Pipelines.gitlab-ci.yml
  - local: '/scripts/ci-helpers/prepare_data_image_job.yml'

.hive_fork_manager_build:
  stage: build_and_test_phase_1
  image: "$CI_REGISTRY_IMAGE/ci-base-image$BUILDER_IMAGE_TAG"
  script:
    - $SETUP_SCRIPTS_PATH/build.sh --cmake-arg="-DHIVE_LINT=OFF" --cmake-arg="-DBUILD_HIVE_TESTNET=$BUILD_HIVE_TESTNET" --haf-source-dir="$CI_PROJECT_DIR" --haf-binaries-dir="$CI_PROJECT_DIR/$CI_JOB_NAME/build" extension.hive_fork_manager query_supervisor unit.pq_utils unit.psql_utils unit.sql_serializer
    - sudo $SETUP_SCRIPTS_PATH/setup_postgres.sh --haf-admin-account=haf_admin --haf-binaries-dir="$CI_PROJECT_DIR/$CI_JOB_NAME/build"
    - cd "$CI_PROJECT_DIR/$CI_JOB_NAME/build" && ctest -j${CTEST_NUMBER_OF_JOBS} --output-on-failure  -R test.functional.hive_fork_manager*
    - cd "$CI_PROJECT_DIR/$CI_JOB_NAME/build" && ctest --output-on-failure  -R test.functional.query_supervisor.*
    - cd "$CI_PROJECT_DIR/$CI_JOB_NAME/build" && ctest --output-on-failure  -R test.unit.*
  artifacts:
    paths:
    - "$CI_JOB_NAME"
    expire_in: 6 hours
  interruptible: true
  tags:
    - public-runner-docker
    - hived

hive_fork_manager:
  variables:
    BUILD_HIVE_TESTNET: "OFF"
  extends: .hive_fork_manager_build

hive_fork_manager_testnet:
  variables:
    BUILD_HIVE_TESTNET: "ON"
  extends: .hive_fork_manager_build

hive_fork_manager_mirrornet:
  variables:
    BUILD_HIVE_TESTNET: "OFF"
  extends: .hive_fork_manager_build

.hived_build:
  stage: build_and_test_phase_1
  image: "$CI_REGISTRY_IMAGE/ci-base-image$BUILDER_IMAGE_TAG"
  script:
    - $SETUP_SCRIPTS_PATH/build.sh --cmake-arg="-DHIVE_LINT=OFF" --haf-source-dir="$CI_PROJECT_DIR" --haf-binaries-dir="$CI_PROJECT_DIR/$CI_JOB_NAME/build" --cmake-arg="-DBUILD_HIVE_TESTNET=$BUILD_HIVE_TESTNET" --cmake-arg="-DHIVE_CONVERTER_BUILD=$HIVE_CONVERTER_BUILD" $HIVE_TARGETS
    # check if sql_serializer compiles with hived
    - test -f "$CI_PROJECT_DIR/$CI_JOB_NAME/build/hive/libraries/plugins/sql_serializer/libsql_serializer_plugin.a"
    # check if sql_serializer plugin is included in hived plugins
    - cd "$CI_PROJECT_DIR/$CI_JOB_NAME/build/hive/programs/hived"
    - ./hived --help | grep psql-url
  artifacts:
    paths:
    - "$CI_JOB_NAME"
    expire_in: 6 hours
  tags:
    - public-runner-docker
    - hived

# job resonsible for hived build in mainnet config.
hived:
  variables:
    BUILD_HIVE_TESTNET: "OFF"
    HIVE_CONVERTER_BUILD: "OFF"
    HIVE_TARGETS: "hived compress_block_log"
  extends: .hived_build

hived_testnet:
  variables:
    BUILD_HIVE_TESTNET: "ON"
    HIVE_CONVERTER_BUILD: "OFF"
    HIVE_TARGETS: "hived cli_wallet get_dev_key compress_block_log"
  extends: .hived_build
  interruptible: true

hived_mirrornet:
  variables:
    BUILD_HIVE_TESTNET: "OFF"
    HIVE_CONVERTER_BUILD: "ON"
    HIVE_TARGETS: "hived cli_wallet get_dev_key compress_block_log"
  extends: .hived_build
  interruptible: true

.pytest_based:
  before_script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):python_venv[collapsed=true]\r\e[0KCreating Python virtual environment..."
      python3 -m venv --system-site-packages venv/
      . venv/bin/activate
      (cd $CI_PROJECT_DIR/tests/integration/haf-local-tools && poetry install)
      echo -e "\e[0Ksection_end:$(date +%s):python_venv\r\e[0K"

haf_system_tests:
  stage: build_and_test_phase_2
  extends: .pytest_based
  needs:
    - job: hive_fork_manager_testnet
      artifacts: true
    - job: hived_testnet
      artifacts: true
  image: "$CI_REGISTRY_IMAGE/ci-base-image$BUILDER_IMAGE_TAG"
  variables:
    HIVE_BUILD_ROOT_PATH: "$CI_PROJECT_DIR/hived_testnet/build/hive"
    DB_NAME: haf_block_log
  script:
    # install hive_fork_manager extension built in previous stage
    - sudo $SETUP_SCRIPTS_PATH/setup_postgres.sh --haf-admin-account=haf_admin --haf-database-store="$CI_PROJECT_DIR/haf_database_store" --haf-binaries-dir="$CI_PROJECT_DIR/hive_fork_manager_testnet/build"
    # setup template database
    - $SETUP_SCRIPTS_PATH/setup_db.sh --haf-db-name="$DB_NAME" --haf-app-user="haf_app_admin"
    # prepare environment and run tests
    - cd $CI_PROJECT_DIR/tests/integration/system/haf
    - pytest --junitxml report.xml -n ${PYTEST_NUMBER_OF_PROCESSES} -m "not mirrornet"
  artifacts:
    paths:
    - "**/generated_during_*"
    - "**/generated_by_package_fixtures"
    reports:
      junit: tests/integration/system/haf/report.xml
    when: always
    expire_in: 1 week
  interruptible: true
  tags:
    - public-runner-docker

dump_snapshot_5m_mirrornet:
  stage: build_and_test_phase_1
  needs:
  - job: "hived_mirrornet"
    artifacts: true
  image: "$CI_REGISTRY_IMAGE/ci-base-image$BUILDER_IMAGE_TAG"
  variables:
    MIRRORNET_WORKING_DIR: "$CI_PROJECT_DIR/mirrornet_witness_node"
  script:
    #Prepare environment for hived run
    - cd $CI_PROJECT_DIR/docker
    - mkdir $MIRRORNET_WORKING_DIR
    - cd $MIRRORNET_WORKING_DIR
    - mkdir blockchain
    - cd blockchain
    - cp $BLOCK_LOG_SOURCE_DIR_MIRRORNET_5M/block_log .
    #Prepare snapshot storage
    - mkdir $SNAPSHOTS_PATH
    - cd $SNAPSHOTS_PATH
    - mkdir 5m_mirrornet
    #Prepare snapshot
    - cd $CI_PROJECT_DIR/hived_mirrornet/build/hive/programs/hived
    - ./hived -d $MIRRORNET_WORKING_DIR --exit-before-sync --replay
    - echo "plugin = state_snapshot" >> $MIRRORNET_WORKING_DIR/config.ini
    - ./hived -d $MIRRORNET_WORKING_DIR --dump-snapshot=snapshot --exit-before-sync
    #Store snapshot in cache
    - mv $MIRRORNET_WORKING_DIR/blockchain $SNAPSHOTS_PATH/5m_mirrornet
    - mv $MIRRORNET_WORKING_DIR/snapshot/snapshot $SNAPSHOTS_PATH/5m_mirrornet

  artifacts:
    paths:
    - "$CI_JOB_NAME"
    expire_in: 6 hours
  tags:
    - data-cache-storage

haf_system_tests_mirrornet:
  stage: build_and_test_phase_2
  extends: .pytest_based
  needs:
    - job: hive_fork_manager_mirrornet
      artifacts: true
    - job: hived_mirrornet
      artifacts: true
    - job: dump_snapshot_5m_mirrornet
      artifacts: false
  image: "$CI_REGISTRY_IMAGE/ci-base-image$BUILDER_IMAGE_TAG"
  variables:
    HIVE_BUILD_ROOT_PATH: "$CI_PROJECT_DIR/hived_mirrornet/build/hive"
    DB_NAME: haf_block_log
  script:
    # copy block_log
    - cp $BLOCK_LOG_SOURCE_DIR_MIRRORNET_5M $CI_PROJECT_DIR/tests/integration/system/haf/mirrornet_tests -r
    # install hive_fork_manager extension built in previous stage
    - sudo $SETUP_SCRIPTS_PATH/setup_postgres.sh --haf-admin-account=haf_admin --haf-database-store="$CI_PROJECT_DIR/haf_database_store" --haf-binaries-dir="$CI_PROJECT_DIR/hive_fork_manager_mirrornet/build"
    # setup template database
    - $SETUP_SCRIPTS_PATH/setup_db.sh --haf-db-name="$DB_NAME" --haf-app-user="haf_app_admin"
    # prepare environment and run tests
    - cd $CI_PROJECT_DIR/tests/integration/system/haf/mirrornet_tests
    - pytest --junitxml report.xml --timeout=3600 --block-log-path=$CI_PROJECT_DIR/tests/integration/system/haf/mirrornet_tests/block_log_5m_mirrornet/block_log --snapshot-path=$SNAPSHOTS_PATH/5m_mirrornet/snapshot -n ${PYTEST_NUMBER_OF_PROCESSES} -m mirrornet

  artifacts:
    paths:
    - "**/generated_during_*"
    - "**/generated_by_package_fixtures"
    exclude:
    - "**/generated_during_*/**/block_log"
    reports:
      junit: tests/integration/system/haf/mirrornet_tests/report.xml
    when: always
    expire_in: 1 week
  interruptible: true
  tags:
    - data-cache-storage

applications_system_tests:
  stage: build_and_test_phase_2
  extends: .pytest_based
  needs:
    - job: hive_fork_manager_testnet
      artifacts: true
    - job: hived_testnet
      artifacts: true
  image: "$CI_REGISTRY_IMAGE/ci-base-image$BUILDER_IMAGE_TAG"
  variables:
    HIVE_BUILD_ROOT_PATH: "$CI_PROJECT_DIR/hived_testnet/build/hive"
  script:
    # install hive_fork_manager extension built in previous stage
    - sudo $SETUP_SCRIPTS_PATH/setup_postgres.sh --haf-admin-account=haf_admin --haf-database-store="$CI_PROJECT_DIR/haf_database_store" --haf-binaries-dir="$CI_PROJECT_DIR/hive_fork_manager_testnet/build"
    # prepare environment and run tests
    - cd $CI_PROJECT_DIR/tests/integration/system/applications
    - pytest --junitxml report.xml -n ${PYTEST_NUMBER_OF_PROCESSES}
  artifacts:
    paths:
    - "**/generated_during_*"
    - "**/generated_by_package_fixtures"
    reports:
      junit: tests/integration/system/haf/report.xml
    when: always
    expire_in: 1 week
  interruptible: true
  tags:
    - public-runner-docker

.replay_step:
  stage: build_and_test_phase_2
  needs:
    - job: hive_fork_manager
      artifacts: true
    - job: hived
      artifacts: true
  image: "$CI_REGISTRY_IMAGE/ci-base-image$BUILDER_IMAGE_TAG"
  variables:
    HIVE_BUILD_ROOT_PATH: "$CI_PROJECT_DIR/hived/build/hive"
    BLOCK_LOG_DIRECTORY: "/blockchain"
    DB_NAME: haf_block_log
    DB_URL: "postgresql:///$DB_NAME"
    DB_ADMIN: "haf_admin"
    REPLAY: "--force-replay"
  before_script:
    - !reference [.pytest_based, before_script]
    - |
      echo -e "\e[0Ksection_start:$(date +%s):replay_setup[collapsed=true]\r\e[0KSetting up replay..."
      sudo $SETUP_SCRIPTS_PATH/setup_postgres.sh --haf-admin-account=$DB_ADMIN --haf-binaries-dir="$CI_PROJECT_DIR/hive_fork_manager/build"
      $SETUP_SCRIPTS_PATH/setup_db.sh --haf-db-name="$DB_NAME" --haf-app-user="haf_app_admin"
      echo -e "\e[0Ksection_end:$(date +%s):replay_setup\r\e[0K"
    # replay
    - |
      echo -e "\e[0Ksection_start:$(date +%s):replay[collapsed=true]\r\e[0KExecuting replay..."
      test -n "$PATTERNS_PATH"
      cd $CI_PROJECT_DIR/tests/integration/replay
      mkdir $PATTERNS_PATH/blockchain
      ls $HIVE_BUILD_ROOT_PATH/programs/util/ -lath
      $HIVE_BUILD_ROOT_PATH/programs/util/compress_block_log --input-block-log $BLOCK_LOG_DIRECTORY --output-block-log $PATTERNS_PATH/blockchain --decompress --block-count 5000000
      $HIVE_BUILD_ROOT_PATH/programs/hived/hived --data-dir $PATTERNS_PATH $REPLAY --exit-before-sync --psql-url $DB_URL 2>&1 | tee -i node_logs.log
      echo -e "\e[0Ksection_end:$(date +%s):replay\r\e[0K"
  script:
    - pytest --junitxml report.xml
  artifacts:
    paths:
    - "**/node_logs.log"
    - "**/node_logs1.log"
    - "**/generated_during_*"
    - "**/generated_by_package_fixtures"
    - "**/*.out.csv"
    reports:
      junit: tests/integration/replay/report.xml
    when: always
    expire_in: 1 week
  interruptible: true
  tags:
    - public-runner-docker
    - hived-for-tests

replay_with_haf:
  extends: .replay_step
  variables:
    PATTERNS_PATH: "$CI_PROJECT_DIR/tests/integration/replay/patterns/no_filter"

replay_accounts_filtered_with_haf:
  extends: .replay_step
  variables:
    PATTERNS_PATH: "$CI_PROJECT_DIR/tests/integration/replay/patterns/accounts_filtered"

replay_accounts_operations_filtered_with_haf:
  extends: .replay_step
  variables:
    PATTERNS_PATH: "$CI_PROJECT_DIR/tests/integration/replay/patterns/accounts_operations_filtered"

replay_virtual_operations_filtered_with_haf:
  extends: .replay_step
  variables:
    PATTERNS_PATH: "$CI_PROJECT_DIR/tests/integration/replay/patterns/virtual_operations_filtered"

replay_operations_filtered_with_haf:
  extends: .replay_step
  variables:
    PATTERNS_PATH: "$CI_PROJECT_DIR/tests/integration/replay/patterns/operations_filtered"

replay_body_operations_filtered_with_haf:
  extends: .replay_step
  variables:
    PATTERNS_PATH: "$CI_PROJECT_DIR/tests/integration/replay/patterns/body_operations_filtered"

replay_accounts_body_operations_filtered_with_haf:
  extends: .replay_step
  variables:
    PATTERNS_PATH: "$CI_PROJECT_DIR/tests/integration/replay/patterns/accounts_body_operations_filtered"

replay_with_update:
  extends: .replay_step
  variables:
    PATTERNS_PATH: "$CI_PROJECT_DIR/tests/integration/replay/patterns/no_filter"
    REPLAY: "--replay-blockchain --stop-replay-at-block 1000000"
    REPLAY_CONTINUATION: "--replay-blockchain --stop-replay-at-block 2000000"
  script:
    # run script that makes database update
    - sudo $CI_PROJECT_DIR/tests/integration/functional/hive_fork_manager/test_extension_update.sh --setup_scripts_path=$SETUP_SCRIPTS_PATH --haf_binaries_dir=$CI_PROJECT_DIR/$CI_JOB_NAME/build --ci_project_dir=$CI_PROJECT_DIR
    # repeat replay from 1 milion blocks
    - $HIVE_BUILD_ROOT_PATH/programs/hived/hived --data-dir $PATTERNS_PATH $REPLAY_CONTINUATION --exit-before-sync --psql-url $DB_URL 2>&1 | tee -i node_logs1.log
    # verify if upgrade is complete by calling the new added function
    - sudo -Enu $DB_ADMIN psql -w --host /var/run/postgresql --port 5432 -d $DB_NAME -v ON_ERROR_STOP=on -U $DB_ADMIN -c "SELECT hive.test()"

update_with_wrong_table_schema:
  extends: .replay_step
  variables:
    PATTERNS_PATH: "$CI_PROJECT_DIR/tests/integration/replay/patterns/no_filter"
    REPLAY: "--replay-blockchain --stop-replay-at-block 1000000"
  script:
    - $CI_PROJECT_DIR/tests/integration/functional/hive_fork_manager/test_table_schema.sh --setup_scripts_path=$SETUP_SCRIPTS_PATH --haf_binaries_dir=$CI_PROJECT_DIR/$CI_JOB_NAME/build --ci_project_dir=$CI_PROJECT_DIR --build_root_dir=$HIVE_BUILD_ROOT_PATH --pattern_dir=$PATTERNS_PATH --psql-url=$DB_URL

block_api_tests: 
  extends: .replay_step
  image: $CI_REGISTRY_IMAGE/ci-base-image:ubuntu22.04-3-jmeter
  variables:
    FF_NETWORK_PER_BUILD: 1
    PATTERNS_PATH: "$CI_PROJECT_DIR/tests/integration/replay/patterns/accounts_body_operations_filtered"
    BENCHMARK_DIR: "$CI_PROJECT_DIR/tests/integration/haf-local-tools/tests_api/benchmarks"
  script:
    # setup
    - |
      echo -e "\e[0Ksection_start:$(date +%s):blocks_api_test_setup[collapsed=true]\r\e[0KSetting up blocks api tests..."
      psql $DB_URL -c "CREATE ROLE bench LOGIN PASSWORD 'mark' INHERIT IN ROLE hived_group;"
      export BENCHMARK_DB_URL="postgresql://bench:mark@localhost:5432/$DB_NAME"
      echo -e "\e[0Ksection_end:$(date +%s):blocks_api_test_setup\r\e[0K"

    # run pattern tests
    - |
      echo -e "\e[0Ksection_start:$(date +%s):blocks_api_test[collapsed=true]\r\e[0KRunning blocks api tests..."
      cd "$BENCHMARK_DIR"
      python3 benchmark.py --loops 200 --threads 5 -n blocks_api -p 5432 -c perf_5M_light.csv --skip-version-check -d wdir --postgres $BENCHMARK_DB_URL --call-style postgres 2>&1 | tee -i $CI_PROJECT_DIR/python_benchmark.log
      echo -e "\e[0Ksection_end:$(date +%s):blocks_api_test\r\e[0K"

    # generate JUNIT report file
    - |
      echo -e "\e[0Ksection_start:$(date +%s):report[collapsed=true]\r\e[0KGenerating Junit report..."
      m2u --input "$BENCHMARK_DIR/wdir/raw_jmeter_report.xml" --output $CI_PROJECT_DIR/report.junit
      echo -e "\e[0Ksection_end:$(date +%s):report\r\e[0K"

  artifacts:
    name: "$CI_JOB_NAME-$CI_COMMIT_REF_NAME"
    reports:
      junit: $CI_PROJECT_DIR/report.junit
    paths:
      - $BENCHMARK_DIR/wdir
      - $CI_PROJECT_DIR/python_benchmark.log
    when: always
    expire_in: 1 week
  tags:
    - public-runner-docker
    - hived-for-tests

prepare_haf_image:
  extends: .prepare_haf_image
  stage: build_and_test_phase_1
  variables:
    REGISTRY_USER: "$CI_IMG_BUILDER_USER"
    REGISTRY_PASS: $CI_IMG_BUILDER_PASSWORD
    BINARY_CACHE_PATH: "haf-binaries"
    HIVE_NETWORK_TYPE: mainnet
  tags:
    - data-cache-storage

prepare_haf_data:
  extends: .prepare_haf_data_5m
  needs:
    - job: prepare_haf_image
      artifacts: true
  stage: build_and_test_phase_1
  variables:
    HIVE_NETWORK_TYPE: mainnet
    BLOCK_LOG_SOURCE_DIR: "$BLOCK_LOG_SOURCE_DIR_5M"
    CONFIG_INI_SOURCE: "$CI_PROJECT_DIR/docker/config_5M.ini"
  tags:
    - data-cache-storage
  resource_group: ${CI_COMMIT_SHA}

.haf-service: &haf-service
  name: $HAF_IMAGE_NAME
  alias: haf-instance
  variables:
    # Allow access from any network to eliminate CI IP addressing problems
    PG_ACCESS: "host    haf_block_log    haf_app_admin    0.0.0.0/0    trust"
    DATADIR: $DATA_CACHE_HAF/datadir
    SHM_DIR: $DATA_CACHE_HAF/shm_dir
    LOG_FILE: $CI_JOB_NAME.log
  command: ["--replay-blockchain", "--stop-replay-at-block=5000000"]

start_haf_as_service:
  stage: build_and_test_phase_2
  image: "$CI_REGISTRY_IMAGE/ci-base-image$BUILDER_IMAGE_TAG"
  needs:
    - job: prepare_haf_data
      artifacts: true
  services:
    - *haf-service
  variables:
    HAF_POSTGRES_URL: postgresql://haf_app_admin@haf-instance:5432/haf_block_log
    HIVED_UID: $HIVED_UID
  script:
    - curl -I haf-instance:8090 || (echo "error connecting to service hived-instance" && false)
    - |
        curl -XPOST -d '{
        "jsonrpc": "2.0",
        "method": "database_api.get_dynamic_global_properties",
        "params": {
        },
        "id": 2
        }' haf-instance:8090
  after_script:
    - rm docker_entrypoint.log -f
    - cp $DATA_CACHE_HAF/datadir/$CI_JOB_NAME.log $CI_PROJECT_DIR/docker_entrypoint.log
  artifacts:
    paths:
    - docker_entrypoint.log
  tags:
    - data-cache-storage
  resource_group: ${CI_COMMIT_SHA}

cleanup_haf_cache_manual:
  extends: .cleanup_cache_manual
  stage: cleanup
  variables:
    CLEANUP_PATH_PATTERN: "/cache/replay_data_haf_*"
  resource_group: ${CI_COMMIT_SHA}
  tags:
    - data-cache-storage

cleanup_haf_snapshot_from_cache:
  extends: .cleanup_cache_manual
  stage: cleanup
  variables:
    CLEANUP_PATH_PATTERN: "/cache/snapshots_pipeline_*"
  resource_group: ${CI_COMMIT_SHA}
  tags:
    - data-cache-storage

build_and_publish_image:
  stage: publish
  extends: .publish_docker_image_template
  script:
    - scripts/ci-helpers/build_and_publish_instance.sh
  tags:
    - public-runner-docker

double_haf_replay_tests:
  extends: .docker_image_builder_job
  stage: build_and_test_phase_2
  variables:
    CONFIG_INI_SOURCE: "$CI_PROJECT_DIR/docker/config_5M.ini"
  needs:
    - job: prepare_haf_image
  script:
  - ./tests/integration/test_double_haf_replay.sh
  tags:
    - hived-for-tests
